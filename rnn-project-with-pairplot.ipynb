{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8664e127",
   "metadata": {},
   "source": [
    "# <img src=\"../assets/logo_infnetv1.png\" alt=\"Infnet logo\" height=\"45\"/> Projeto de Disciplina de Redes Neurais com TensorFlow\n",
    "<img src=\"https://img.shields.io/badge/python-v._3.11.5-blue?style=flat-square&logo=python&logoColor=white\" alt=\"python_logo\" height=\"20\"/>\n",
    "<img src=\"https://img.shields.io/badge/jupyter-v._5.7.2-blue?style=flat-square&logo=jupyter&logoColor=white\" alt=\"jupyter_logo\" height=\"20\"/>\n",
    "<img src=\"https://img.shields.io/badge/anaconda-v._23.7.4-blue?style=flat-square&logo=anaconda&logoColor=white\" alt=\"anaconda_logo\" height=\"20\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756d321f",
   "metadata": {},
   "source": [
    "#### Grupo: \n",
    "\n",
    "- Mateus Teixeira Ramos da Silva <a href=\"https://github.com/GitMateusTeixeira/03-ml-modeling\"><img src=\"https://img.shields.io/badge/Github-151b23?style=flat-square&logo=github\" alt=\"github_logo\" height=\"20\"/> </a>\n",
    "- aaaa\n",
    "- aaaa\n",
    "- aaaa\n",
    "- aaaa\n",
    "\n",
    "### Sobre o projeto:\n",
    "\n",
    "---\n",
    "\n",
    "Se trata de um modelo de aprendizagem suprvisionado de classifica√ß√£o bin√°ria envolvendo dados relativos ao C√¢ncer de Mama. Os dados foram extra√≠dos do site do <a href=\"https://www.kaggle.com/datasets/wasiqaliyasir/breast-cancer-dataset\">`Kaggle`</a>.\n",
    "\n",
    "#### Conven√ß√µes de reprodutibilidade:\n",
    "\n",
    "- Todas as bibliotecas se encontram no arquivo `üìÑrequirements.txt`\n",
    "- Para mais inform√ß√µes, consulte nosso `README`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcfc993",
   "metadata": {},
   "source": [
    "### √çndice\n",
    "\n",
    "---\n",
    "\n",
    "- <a href='#parte-01-importar-os-pacotes'>Parte 01. Importar os pacotes</a>\n",
    "\n",
    "- <a href='#parte-02-baixar-e-ler-os-dados'>Parte 02. Baixar e ler os dados</a>\n",
    "\n",
    "- <a href='#parte-03-an√°lise-explorat√≥ria'>Parte 03. An√°lise explorat√≥ria</a>\n",
    "\n",
    "- <a href='#parte-04-tratamento-dos-dados-exclus√£o-da-coluna-nula'>Parte 04. Tratamento dos dados</a>\n",
    "\n",
    "- <a href='#parte-05-verificando-a-correla√ß√£o-entre-as-colunas'>Parte 05. Verificando a correla√ß√£o entre as colunas</a>\n",
    "\n",
    "- <a href='#parte-06-separar-as-features-utilizadas'>Parte 06. Separar as features</a>\n",
    "\n",
    "- <a href='#parte-07-normaliza√ß√£o-com-o-standard-scaller'>Parte 07. Normaliza√ß√£o com o Standard Scaller</a>\n",
    "\n",
    "- <a href='#parte-08-modelos-baseline'>Parte 08. Modelos baseline</a>\n",
    "\n",
    "- - <a href='#81-criando-um-dumb-model-baseline-prevendo-tudo-como-a-classe-majorit√°ria-para-valida√ß√µes-futuras'>8.1. Dumb model</a>\n",
    "\n",
    "- - <a href='#82-criando-um-baseline-de-keras-padr√£o-sem-nenhum-finetuning'>8.2. Baseline com Keras padr√£o</a>\n",
    "\n",
    "- <a href='#parte-09-grid-search-com-valida√ß√£o-cruzada'>Parte 09. Grid search com valida√ß√£o cruzada</a>\n",
    "\n",
    "- <a href='#parte-10-compara√ß√£o-dos-modelos'>Parte 10. Compara√ß√£o dos modelos</a>\n",
    "\n",
    "- <a href='#parte-11-streamlit'>Parte 11. Streamlit</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fabcf2",
   "metadata": {},
   "source": [
    "## Parte 01. Importar os pacotes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, \\\n",
    "                            confusion_matrix, roc_curve, roc_auc_score, auc, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "# Silenciar os avisos do c√≥digo\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3381b1",
   "metadata": {},
   "source": [
    "## Parte 02. Baixar e ler os dados\n",
    "\n",
    "---\n",
    "\n",
    "<a href='#√≠ndice'>Voltar ao in√≠cio</a>\n",
    "\n",
    "O arquivo ser√° baixado diretamente do reposit√≥rio do Kaggle. Caso a estrutura de pastas n√£o exista, o algoritmo ir√° constru√≠-la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466681da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar a API do Kaggle\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Criar o diret√≥rio se n√£o existir\n",
    "data_path = '../data/01-raw'\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Baixar os dados do Kaggle\n",
    "api.dataset_download_files('wasiqaliyasir/breast-cancer-dataset', path=data_path, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67885663",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw = '../data/01-raw'\n",
    "file_raw = 'Breast_cancer_dataset.csv'\n",
    "\n",
    "pathfile_raw = os.path.join(path_raw, file_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847cedee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(pathfile_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab0632",
   "metadata": {},
   "source": [
    "## Parte 03. An√°lise explorat√≥ria\n",
    "\n",
    "---\n",
    "\n",
    "<a href='#√≠ndice'>Voltar ao in√≠cio</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"O dataset possui {df.shape[0]} linhas e {df.shape[1]} colunas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f2d93",
   "metadata": {},
   "source": [
    "### 3.1. Vis√£o geral do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec0d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detalhando a base de dados com moda\n",
    "def check(df):\n",
    "    l = []\n",
    "    colunas = df.columns\n",
    "    \n",
    "    for col in colunas:\n",
    "        dtypes = df[col].dtypes\n",
    "        nunique = df[col].nunique()\n",
    "        sum_null = df[col].isnull().sum()\n",
    "\n",
    "        # Calcula a moda e a frequ√™ncia da moda\n",
    "        moda = df[col].mode().iloc[0] if not df[col].mode().empty else \"N√£o se aplica\"\n",
    "        moda_freq = df[col].value_counts().iloc[0] if not df[col].value_counts().empty else \"N√£o se aplica\"\n",
    "\n",
    "        if np.issubdtype(dtypes, np.number):\n",
    "            status = df.describe(include='all').T\n",
    "            media = status.loc[col, 'mean']\n",
    "            std = status.loc[col, 'std']\n",
    "            min_val = status.loc[col, 'min']\n",
    "            quar1 = status.loc[col, '25%']\n",
    "            mediana = df[col].median()\n",
    "            quar3 = status.loc[col, '75%']\n",
    "            max_val = status.loc[col, 'max']\n",
    "                    \n",
    "        else:\n",
    "            status = \"N√£o se aplica\"\n",
    "            media = \"N√£o se aplica\"\n",
    "            std = \"N√£o se aplica\"\n",
    "            min_val = \"N√£o se aplica\"\n",
    "            quar1 = \"N√£o se aplica\"\n",
    "            mediana = \"N√£o se aplica\"\n",
    "            quar3 = \"N√£o se aplica\"\n",
    "            max_val = \"N√£o se aplica\"\n",
    "\n",
    "        l.append([col, dtypes, nunique, sum_null, media, std, min_val, quar1, mediana, quar3, max_val, moda, moda_freq])\n",
    "    \n",
    "    # Cria√ß√£o do DataFrame com as novas colunas\n",
    "    df_check = pd.DataFrame(l)\n",
    "    df_check.columns = ['coluna', 'tipo', '√∫nicos', 'null_soma', 'media', 'desvio', \n",
    "                        'minimo', '25%', 'mediana', '75%', 'maximo', 'moda', 'frequ√™ncia_moda']\n",
    "    \n",
    "    return df_check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f510a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise geral dos dados\n",
    "check(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise dos tipos das colunas\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea8c0c",
   "metadata": {},
   "source": [
    "√â poss√≠vel verificar que o dataset original possui 33 colunas, das quais uma ('Unamed: 32') possui apenas dados nulos.\n",
    "\n",
    "Al√©m disso, com exece√ß√£o da coluna 'diagnosis' (que √© uma coluna categ√≥rica), as demais colunas s√£o colunas num√©ricas, com informa√ß√µes sobre cada paciente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14270985",
   "metadata": {},
   "source": [
    "### 3.2. An√°lise da distribui√ß√£o dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pairplot_section",
   "metadata": {},
   "source": [
    "### 3.3. Pairplot - An√°lise de Relacionamentos entre Vari√°veis\n",
    "\n",
    "O **Pairplot** (ou scatterplot matrix) √© uma ferramenta fundamental na an√°lise explorat√≥ria de dados (EDA). \n",
    "Este gr√°fico mostra, para cada par de vari√°veis num√©ricas, como elas se relacionam, al√©m da distribui√ß√£o individual \n",
    "em forma de histograma/densidade na diagonal.\n",
    "\n",
    "**Para que serve:**\n",
    "\n",
    "- **Explora√ß√£o de dados (EDA)**: Antes de treinar uma rede neural (ou qualquer modelo de machine learning), \n",
    "  √© essencial entender como os dados est√£o distribu√≠dos, se existem correla√ß√µes fortes entre vari√°veis e \n",
    "  se h√° separabilidade entre classes.\n",
    "\n",
    "- **Redu√ß√£o de dimensionalidade / sele√ß√£o de features**: O pairplot ajuda a identificar vari√°veis altamente \n",
    "  correlacionadas (como radius_mean, perimeter_mean e area_mean), que podem ser redundantes. Isso √© √∫til \n",
    "  porque redes neurais podem sofrer com dados redundantes ou multicolinearidade.\n",
    "\n",
    "- **Identifica√ß√£o de padr√µes de separa√ß√£o**: Voc√™ pode ver se as classes se separam visualmente em \n",
    "  determinados pares de features. Isso d√° uma intui√ß√£o de quais vari√°veis carregam mais poder discriminativo.\n",
    "\n",
    "**Quando usar em rela√ß√£o a uma rede neural:**\n",
    "\n",
    "- **Antes do treinamento**: Para inspecionar os dados, escolher features relevantes e entender poss√≠veis \n",
    "  ajustes de pr√©-processamento (normaliza√ß√£o, remo√ß√£o de redund√¢ncias).\n",
    "\n",
    "- **N√£o durante o treinamento**: O pairplot √© puramente explorat√≥rio; n√£o entra como input em uma rede neural. \n",
    "  A rede usar√° os valores num√©ricos das features (normalizados ou padronizados), n√£o o gr√°fico em si.\n",
    "\n",
    "**Fluxo t√≠pico:**\n",
    "\n",
    "1. EDA com pairplot ‚Üí 2. Pr√©-processamento (scaling, sele√ß√£o de features, balanceamento de classes) ‚Üí \n",
    "3. Treino da rede neural (ou outro modelo).\n",
    "\n",
    "Este gr√°fico √© como o 'raio-X' inicial dos dados, antes de colocar a rede para aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pairplot_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro, vamos criar uma coluna num√©rica para o diagn√≥stico\n",
    "df_pairplot = df.copy()\n",
    "\n",
    "# Codificar a vari√°vel target\n",
    "label_encoder = LabelEncoder()\n",
    "df_pairplot['diagnosis_encoded'] = label_encoder.fit_transform(df_pairplot['diagnosis'])\n",
    "\n",
    "# Selecionar algumas vari√°veis principais para o pairplot (para n√£o sobrecarregar o gr√°fico)\n",
    "features_for_pairplot = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'diagnosis_encoded']\n",
    "\n",
    "# Criar o subset dos dados\n",
    "df_subset = df_pairplot[features_for_pairplot]\n",
    "\n",
    "print(f\"Vari√°veis selecionadas para o Pairplot: {features_for_pairplot[:-1]}\")\n",
    "print(f\"Classes: 0 = Benigno ({sum(df_pairplot['diagnosis_encoded'] == 0)} casos), 1 = Maligno ({sum(df_pairplot['diagnosis_encoded'] == 1)} casos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pairplot_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar o estilo do seaborn\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Criar o pairplot\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Pairplot com separa√ß√£o por classe\n",
    "pairplot = sns.pairplot(\n",
    "    df_subset, \n",
    "    hue='diagnosis_encoded',\n",
    "    diag_kind='hist',\n",
    "    plot_kws={'alpha': 0.6, 's': 30},\n",
    "    diag_kws={'alpha': 0.7}\n",
    ")\n",
    "\n",
    "# Personalizar o gr√°fico\n",
    "pairplot.fig.suptitle('Pairplot - An√°lise de Relacionamentos entre Vari√°veis do C√¢ncer de Mama', \n",
    "                      fontsize=16, y=1.02)\n",
    "\n",
    "# Ajustar as legendas\n",
    "handles = pairplot._legend_data.values()\n",
    "labels = ['Benigno (0)', 'Maligno (1)']\n",
    "pairplot.fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(0.98, 0.98))\n",
    "\n",
    "# Remover a legenda original\n",
    "pairplot._legend.remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pairplot_analysis",
   "metadata": {},
   "source": [
    "**An√°lise do Pairplot:**\n",
    "\n",
    "1. **Correla√ß√µes Fortes**: Observamos correla√ß√µes muito fortes entre `radius_mean`, `perimeter_mean` e `area_mean`, \n",
    "   o que √© esperado geometricamente (raio, per√≠metro e √°rea est√£o matematicamente relacionados).\n",
    "\n",
    "2. **Separabilidade das Classes**: \n",
    "   - Tumores malignos (classe 1) tendem a ter valores maiores de raio, per√≠metro e √°rea\n",
    "   - Existe uma boa separa√ß√£o visual entre as classes em v√°rias combina√ß√µes de vari√°veis\n",
    "   - A textura tamb√©m mostra alguma capacidade discriminativa\n",
    "\n",
    "3. **Distribui√ß√µes**: \n",
    "   - As distribui√ß√µes na diagonal mostram que algumas vari√°veis podem se beneficiar de normaliza√ß√£o\n",
    "   - H√° evid√™ncia de que as classes t√™m distribui√ß√µes diferentes para a maioria das vari√°veis\n",
    "\n",
    "4. **Implica√ß√µes para a Rede Neural**:\n",
    "   - A forte correla√ß√£o entre algumas vari√°veis sugere que podemos considerar redu√ß√£o de dimensionalidade\n",
    "   - A boa separabilidade visual indica que uma rede neural deve conseguir aprender padr√µes discriminativos\n",
    "   - A necessidade de normaliza√ß√£o √© evidente devido √†s diferentes escalas das vari√°veis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea8c0c",
   "metadata": {},
   "source": [
    "√â poss√≠vel verificar que o dataset original possui 33 colunas, das quais uma ('Unamed: 32') possui apenas dados nulos.\n",
    "\n",
    "Al√©m disso, com exece√ß√£o da coluna 'diagnosis' (que √© uma coluna categ√≥rica), as demais colunas s√£o colunas num√©ricas, com informa√ß√µes sobre cada paciente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14270985",
   "metadata": {},
   "source": [
    "### 3.2. An√°lise da distribui√ß√£o dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c7b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise da distribui√ß√£o da vari√°vel target\n",
    "print(f\"Distribui√ß√£o da vari√°vel target:\")",
    "\nprint(df['diagnosis'].value_counts())",
    "\nprint(f\"\\nPercentual da distribui√ß√£o da vari√°vel target:\")",
    "\nprint(df['diagnosis'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_target_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o da distribui√ß√£o da vari√°vel target\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='diagnosis', palette='viridis')\n",
    "plt.title('Distribui√ß√£o da Vari√°vel Target (Diagnosis)')\n",
    "plt.xlabel('Diagn√≥stico')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tratamento_dados",
   "metadata": {},
   "source": [
    "## Parte 04. Tratamento dos dados (exclus√£o da coluna nula)\n",
    "\n",
    "---\n",
    "\n",
    "<a href='#√≠ndice'>Voltar ao in√≠cio</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remove_null_column",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo a coluna com valores nulos\n",
    "df = df.drop('Unnamed: 32', axis=1)\n",
    "print(f\"Novo shape do dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correlacao",
   "metadata": {},
   "source": [
    "## Parte 05. Verificando a correla√ß√£o entre as colunas\n",
    "\n",
    "---\n",
    "\n",
    "<a href='#√≠ndice'>Voltar ao in√≠cio</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encode_target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificando a vari√°vel target para an√°lise de correla√ß√£o\n",
    "le = LabelEncoder()\n",
    "df['diagnosis_encoded'] = le.fit_transform(df['diagnosis'])\n",
    "print(f\"Mapeamento: {dict(zip(le.classes_, le.transform(le.classes_)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correla√ß√£o\n",
    "plt.figure(figsize=(20, 16))\n",
    "correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Matriz de Correla√ß√£o das Vari√°veis Num√©ricas')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separar_features",
   "metadata": {},
   "source": [
    "## Parte 06. Separar as features utilizadas\n",
    "\n",
    "---\n",
    "\n",
    "<a href='#√≠ndice'>Voltar ao in√≠cio</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando features e target\n",
    "X = df.drop(['id', 'diagnosis', 'diagnosis_encoded'], axis=1)\n",
    "y = df['diagnosis_encoded']\n",
    "\n",
    "print(f\"Shape das features (X): {X.shape}\")\n",
    "print(f\"Shape do target (y): {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normalizacao",
   "metadata": {},
   "source": [
    "## Parte 07. Normaliza√ß√£o com o Standard Scaler\n",
    "\n",
    "---\n",
    "\n",
    "<a href='#√≠ndice'>Voltar ao in√≠cio</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divis√£o treino/teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "\n",
    "print(f\"Shape X_train: {X_train.shape}\")\n",
    "print(f\"Shape X_test: {X_test.shape}\")\n",
    "print(f\"Shape y_train: {y_train.shape}\")\n",
    "print(f\"Shape y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza√ß√£o dos dados\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"M√©dia antes da normaliza√ß√£o: {X_train.mean().mean():.4f}\")\n",
    "print(f\"Desvio padr√£o antes da normaliza√ß√£o: {X_train.std().mean():.4f}\")\n",
    "print(f\"M√©dia ap√≥s a normaliza√ß√£o: {X_train_scaled.mean():.4f}\")\n",
    "print(f\"Desvio padr√£o ap√≥s a normaliza√ß√£o: {X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modelos_baseline",
   "metadata": {},
   "source": [
    "## Parte 08. Modelos baseline\n",
    "\n",
    "---\n",
    "\n",
    "<a href='#√≠ndice'>Voltar ao in√≠cio</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dumb_model",
   "metadata": {},
   "source": [
    "### 8.1. Criando um dumb model (baseline) prevendo tudo como a classe majorit√°ria para valida√ß√µes futuras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dumb_baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumb model - sempre prediz a classe majorit√°ria\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_model = DummyClassifier(strategy='most_frequent', random_state=SEED)\n",
    "dummy_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predi√ß√µes\n",
    "y_pred_dummy = dummy_model.predict(X_test_scaled)\n",
    "\n",
    "# M√©tricas\n",
    "print(\"=== DUMB MODEL (Baseline) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dummy):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_dummy):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_dummy):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_dummy):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "keras_baseline",
   "metadata": {},
   "source": [
    "### 8.2. Criando um baseline de Keras padr√£o (sem nenhum fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "keras_baseline_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo baseline com Keras\n",
    "def create_baseline_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Criando e treinando o modelo\n",
    "baseline_model = create_baseline_model()\n",
    "\n",
    "# Treinamento\n",
    "history_baseline = baseline_model.fit(X_train_scaled, y_train,\n",
    "                                     epochs=100,\n",
    "                                     batch_size=32,\n",
    "                                     validation_split=0.2,\n",
    "                                     verbose=0)\n",
    "\n",
    "# Predi√ß√µes\n",
    "y_pred_baseline_prob = baseline_model.predict(X_test_scaled)\n",
    "y_pred_baseline = (y_pred_baseline_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# M√©tricas\n",
    "print(\"=== KERAS BASELINE MODEL ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_baseline):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_baseline):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_baseline):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_baseline):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grid_search",
   "metadata": {},
   "source": [
    "## Parte 09. Grid search com valida√ß√£o cruzada\n",
    "\n",
    "---\n",
    "\n",
    "<a href='#√≠ndice'>Voltar ao in√≠cio</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid_search_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para criar modelo para grid search\n",
    "def create_model(neurons=64, learning_rate=0.001, dropout_rate=0.2):\n",
    "    model = Sequential([\n",
    "        Dense(neurons, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(neurons//2, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid_search_execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search com valida√ß√£o cruzada\n",
    "keras_classifier = KerasClassifier(model=create_model, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Par√¢metros para grid search\n",
    "param_grid = {\n",
    "    'model__neurons': [32, 64, 128],\n",
    "    'model__learning_rate': [0.001, 0.01],\n",
    "    'model__dropout_rate': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(estimator=keras_classifier,\n",
    "                          param_grid=param_grid,\n",
    "                          cv=3,\n",
    "                          scoring='f1',\n",
    "                          n_jobs=-1,\n",
    "                          verbose=1)\n",
    "\n",
    "# Executar grid search\n",
    "print(\"Executando Grid Search...\")\n",
    "grid_result = grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Melhores par√¢metros\n",
    "print(f\"Melhores par√¢metros: {grid_result.best_params_}\")\n",
    "print(f\"Melhor score: {grid_result.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best_model_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia√ß√£o do melhor modelo\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "# Predi√ß√µes\n",
    "y_pred_best_prob = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# M√©tricas\n",
    "print(\"=== MELHOR MODELO (Grid Search) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_best_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparacao_modelos",
   "metadata": {},
   "source": [
    "## Parte 10. Compara√ß√£o dos modelos\n",
    "\n",
    "---\n",
    "\n",
    "<a href='#√≠ndice'>Voltar ao in√≠cio</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara√ß√£o dos modelos\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Modelo': ['Dumb Classifier', 'Keras Baseline', 'Melhor Modelo (Grid Search)'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_dummy),\n",
    "        accuracy_score(y_test, y_pred_baseline),\n",
    "        accuracy_score(y_test, y_pred_best)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test, y_pred_dummy),\n",
    "        precision_score(y_test, y_pred_baseline),\n",
    "        precision_score(y_test, y_pred_best)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test, y_pred_dummy),\n",
    "        recall_score(y_test, y_pred_baseline),\n",
    "        recall_score(y_test, y_pred_best)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test, y_pred_dummy),\n",
    "        f1_score(y_test, y_pred_baseline),\n",
    "        f1_score(y_test, y_pred_best)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== COMPARA√á√ÉO DOS MODELOS ===\")\n",
    "print(models_comparison.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion_matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrizes de confus√£o\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "models_preds = [y_pred_dummy, y_pred_baseline, y_pred_best]\n",
    "model_names = ['Dumb Classifier', 'Keras Baseline', 'Melhor Modelo']\n",
    "\n",
    "for i, (pred, name) in enumerate(zip(models_preds, model_names)):\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "    axes[i].set_title(f'Matriz de Confus√£o - {name}')\n",
    "    axes[i].set_xlabel('Predito')\n",
    "    axes[i].set_ylabel('Real')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc_curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# ROC para baseline\n",
    "fpr_baseline, tpr_baseline, _ = roc_curve(y_test, y_pred_baseline_prob.flatten())\n",
    "auc_baseline = auc(fpr_baseline, tpr_baseline)\n",
    "\n",
    "# ROC para melhor modelo\n",
    "fpr_best, tpr_best, _ = roc_curve(y_test, y_pred_best_prob)\n",
    "auc_best = auc(fpr_best, tpr_best)\n",
    "\n",
    "# Plot\n",
    "plt.plot(fpr_baseline, tpr_baseline, label=f'Keras Baseline (AUC = {auc_baseline:.3f})', linewidth=2)\n",
    "plt.plot(fpr_best, tpr_best, label=f'Melhor Modelo (AUC = {auc_best:.3f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Linha de Base (AUC = 0.5)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falsos Positivos')\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "plt.title('Curvas ROC - Compara√ß√£o dos Modelos')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streamlit_section",
   "metadata": {},
   "source": [
    "## Parte 11. Streamlit\n",
    "\n",
    "---\n",
    "\n",
    "<a href='#√≠ndice'>Voltar ao in√≠cio</a>\n",
    "\n",
    "*Nota: A implementa√ß√£o do Streamlit ser√° feita apenas na pipeline de produ√ß√£o.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}